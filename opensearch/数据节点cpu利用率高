
尊贵的客户，您好！

我们了解到您的OpenSearch集群lfl-infra-opensearch观察到数据节点3THh7rbAQdCg-E7CXzShFg出现接近100%的cpu使用率的情况，需要协助确认原因。

从CloudWatch的指标上来看，节点3THh7rbAQdCg-E7CXzShFg在UTC时间2024-02-22 07:30左右开始出现cpu升高的情况，并且一直持续。
而另外一个数据节点xtgsMEJmS4aGmjui1_B9og则维持相对较低的CPU使用率，绝大多数时间使用率低于50%：（ES/OpenSearchService • CPUUtilization 1mins Maxmium）

进而查看_node hot_threads API的输出，我们可以确认该节点正在执行索引的写入操作，如2024-02-22T16:11:12 UTC时刻此节点上共有3个hot thread，均为indexing：
   
98.6% (492.9ms out of 500ms) cpu usage by thread 'opensearch[bce7af186d1c33d743dd02a684cc8179][write][T#3]'
...
86.3% (431.2ms out of 500ms) cpu usage by thread 'opensearch[bce7af186d1c33d743dd02a684cc8179][write][T#2]'
...
83.8% (418.9ms out of 500ms) cpu usage by thread 'opensearch[bce7af186d1c33d743dd02a684cc8179][write][T#1]'

经过多次观察，hot thread均为写入线程，并且用cat tasks API多次看到该节点运行写入操作均为bulk操作。可以排除您在案例中怀疑的索引向温节点迁移或者分片操作导致的。

进一步查看，我们注意到集群中的一个索引eip-log-as1-2024.02.22有异常的表现。
首先，从cat indices输出来看，eip-log-as1索引缺少2024.02.21日的数据，而eip-log-as1-2024.02.22的数据从占用空间以及写入的document数量均高于倍以上：
...
green  open   eip-log-as1-2024.02.19        higZwAJ0RPKTZfm76WW6Zg   1   1   31549740            0     10.8gb          5.4gb
green  open   eip-log-as1-2024.02.20        2Ji9TTydSiyJJv7rVEMEmw   1   1    8786405            0        3gb          1.5gb
green  open   eip-log-as1-2024.02.22        h30qWrdTSX-o7IesL_1trw   1   1   31121580            0     14.9gb          9.4gb <---缺少21日的索引，占用空间及document数量均高于往日
...

使用_cat shards/eip-log-as1-2024.02.22?h=index,shard,prirep,state,docs,store,id API查看此索引的主分片，确认是在数据节点3THh7rbAQdCg-E7CXzShFg上：
index                      shard prirep state       docs store id
eip-log-as1-2024.02.22 0     p      STARTED 34736273 6.3gb 3THh7rbAQdCg-E7CXzShFg
eip-log-as1-2024.02.22 0     r      STARTED 34648076 6.2gb xtgsMEJmS4aGmjui1_B9og

综合以上发现，我们认为是由于索引eip-log-as1-2024.02.22的分片策略只分了一个分片，并且该索引今日的写入量级高于往日数倍，所以导致该分片所在的数据节点CPU使用率的升高。
如果该索引今日写入增多的原因是包含了21日的数据的话，待今日的写入完成后再观察一下23日的写入大小是否恢复到平时的水准。
如果情况没有改善的话，建议您将该索引设置为2个分片，将写入请求的负载平均分担到两个节点上，对于其他的热索引，我们也推荐相同的操作。

另外我们注意到的一个情况是，这两个数据节点每个都有963个。您的机型是r6g.xlarge的机型，共有32G内存。其中50%的内存会被划分为java的heap内存，即16G。
根据OpenSearch服务的最佳实践的推荐，每个节点的shard数量是每GB Java heap内存可分配25个shard，所以您的机型推荐每个节点的shard数量是16*25=400.
您当前的设置已经超过推荐值2倍多，并且已经接近单个节点shard数量的最大值1000个，请考虑将多个索引合并，或者删除不用的索引以减少单个节点的索引数量，抑或增加数据节点的数量，摊平每节点的shard数量。

[1] https://docs.aws.amazon.com/zh_cn/opensearch-service/latest/developerguide/bp.html#bp-shard-count  : Amazon OpenSearch Service 的运行最佳实践 - 确定分片和数据节点数

#########################################################################################

尊敬的亚马逊云用户，您好！

首先数据节点3THh7rbAQdCg-E7CXzShFg上面观察到的从UTC时间2024-02-22 07:30左右开始出现cpu使用率持续击穿95%以上的现象，经过我们当时收集到的操作系统层面进程cpu使用率的信息判断已经确定是OpenSearch进程产生的CPU高使用率的占用，而非操作系统层面的其他进程导致的。其次，从02/22-23日期间多次运行的hot_threads API的输出可以确定，当天的hot thread都是索引写入的线程，从cat tasks API来看，应该是bulk写入操作。而且依据以上信息，当天（22日）的hot index是lfl-eip-log-as1-2024.02.22。但是在23日的时候22日的日志写入应该已经完成，令人感到异常的是此节点的高cpu的现象却没有消除。直至今日，我们已经有了24日、25日以及26日的数据，可以看到高cpu现象的变化规律。

1. 2024-02-22 07:30左右开始您的某些更改或者索引的新增导致当时的hot index eip-log-as1-2024.02.22有持续性的大量的写入，而该索引只划分了一个primary shard，由于索引的写入操作只会向primary shard写入，所以该索引的primary shard所在的数据节点CPU负载变高。
2. 该节点的cpu使用率在23日依旧频繁的击穿95%以上，直至2024-02-24 UTC零点以后，另外一个数据节点xtgsMEJmS4aGmjui1_B9og开始出现高CPU使用率的情况，而之前高CPU的节点3THh7rbAQdCg-E7CXzShFg的cpu使用率则在40%与70%之间波动；
3. UTC时间25日零点之后，两个数据节点的CPU使用率又开始交替变化，直至26日零点之后。

由于您的这个集群中的几乎所有的索引分分片策略都是1:1，即每个索引只有一个primary shard，所以某个或者某些索引的高压力写入都会导致该索引primary shard所在的节点资源使用率比另外一个高。
判断集群中的hot index需要根据您的OpenSearch域的客户端的写入情况进行估算，否则的话就只能几个小时的持续观察索引的document以及存储空间的变化情况，比较耗时。
但是有了上述我们已经观察到了23-26日的两个节点的cpu交替变化的情况，我们可以确认导致此问题的原因是索引的shard划分不合理导致的。

所以之后的建议需要请您重新审核您的每个索引，
1. 删除不必要的索引，保证每个节点分配到的shard数量不超过推荐值400个，这可以显著减少内存使用率的占用情况；
2. 将每个索引的主分片数量调整到数据节点数量一致，也就是2或者是2的倍数，这样可以保证两个数据节点负载大致相同；

只有小索引，如索引的占用空间是KB或者MB数量级的索引，可以不必修改其主分片的数量，以期减少整个集群中的总分片数量。在完成上述两点的建议之后，您应该观察到这两个节点的cpu和内存使用率大致相当。如果此时您还会观察到cpu使用率长时间超过80%以上，或者JVM memory pressure持续75%以上，则建议您考虑增加数据节点的数量。

#########################################################################################

另外两个数据节点的可用空间不足373555MB，可以查看我们hot往warm每天正常迁移吗？迁移以后空间没降下来？只能删除索引？

#########################################################################################

尊敬的亚马逊云用户，您好！


我查看了集群的这两个data node的02/21/2024 - 02/28/2024最近1周的FreeStorageSpace的指标(1min Minimum)，可以观察到在每天的UTC时间1点左右都能看到FreeStorageSpace的指标值上升，
说明在当时有集群索引的清理工作，从每天定时清理来看，应该是您的集群的ISM将索引从data node往warm node迁移的任务之行的结果。建议您结合您的当前数据节点中的索引数量和创建时间，结合您的ISM policy设置进行验证。
但是，从指标上来看，过去1周之内清理之后的最高的存储可用空间却从400G左右逐渐下降到360G左右，说明每天的写入数据量要大于迁移出去的数据量，
如果这种趋势一直持续下去，很快您的集群就会出现磁盘空间紧张的情况出现.

但是本案例中之前回复建议您删除不必要的索引的建议，是因为此集群中的总的shard数量很大，导致每个数据节点已经达到943个，非常接近单个节点能够容纳的shard数量的最大值1000，
请参考如下cat allocation输出结果：

shards disk.indices disk.used disk.avail disk.total disk.percent host          ip            node
   943        1.4tb     1.4tb    348.6gb      1.7tb           80 172.16.17.179 172.16.17.179 7781757cf3240a77cc9c98d8b7ffe7ea
   943        1.4tb     1.4tb    345.2gb      1.7tb           80 172.16.43.8   172.16.43.8   bce7af186d1c33d743dd02a684cc8179

并且，根据这个案例中第一个回复中向您解释的，根据您的数据节点的内存大小，AWS推荐每个节点最大的shard数量是400个。
依据以上情况之前回复中建议您删除不必要的索引。如果从业务上看，存储的数据量逐渐增长是业务的正常增长，索引数量无法减少的话，您就只能向集群中添加数据节点的数量。
按照现在的集群的情况，您需要向集群中再增加2个数据节点才能保证每个数据节点的shard数量小于400个。同时，这个办法也可以顺带解决集群存储空间紧张的问题。

